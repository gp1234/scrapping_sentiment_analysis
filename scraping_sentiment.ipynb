{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping and Sentiment Analysis example\n",
    "In the code snippet provided, the following actions are performed:\n",
    "\n",
    "1. The necessary modules are imported: `requests`, `re`, and `BeautifulSoup` from the `bs4` library.\n",
    "2. A function named `scrape_data` is defined. This function takes a URL as input and performs web scraping to extract data from the webpage.\n",
    "3. Inside the `scrape_data` function, a user agent header is set to mimic a web browser.\n",
    "4. A GET request is made to the specified URL using the `requests.get()` method, along with the headers.\n",
    "5. The response is parsed using BeautifulSoup with the \"html.parser\" parser.\n",
    "6. The function finds all the \"ul\" elements with the class \"feed-chronology\" and stores them in the `lastest_news` variable.\n",
    "7. A loop iterates over each news item and extracts the links.\n",
    "8. The extracted links are appended to the `links_arr` list after modifying them to include the base URL.\n",
    "9. Another GET request is made to the first link in the `links_arr` list.\n",
    "10. The response is parsed using BeautifulSoup.\n",
    "11. The function finds all the \"p\" elements with the class \"prose-text\" and stores them in the `content` variable.\n",
    "12. The text content of each \"p\" element is concatenated and stored in the `text_to_analyze` variable.\n",
    "13. Finally, the `text_to_analyze` variable is returned as the result of the `scrape_data` function.\n",
    "\n",
    "Below the code snippet, the `scrape_data` function is called with the URL _blank to scrape data from that webpage. The extracted text is assigned to the `text_to_analyze` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "def scrape_data(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    results = BeautifulSoup(response.text, \"html.parser\")\n",
    "    lastest_news = results.find_all(\"ul\", class_=\"feed-chronology\")\n",
    "    for news in lastest_news:\n",
    "        links = news.findAll(\"a\")\n",
    "    links_arr = []\n",
    "    for link in links:\n",
    "         links_arr.append(re.sub(r\"/$\", \"\", url) + link.get(\"href\"))\n",
    "    response_2 = requests.get(links_arr[0], headers=headers)\n",
    "    results_2 = BeautifulSoup(response_2.text, \"html.parser\")\n",
    "    content = results_2.find_all(\"p\", class_=\"prose-text\")\n",
    "    text_to_analyze = \"\"\n",
    "    for c in content:\n",
    "        text_to_analyze += c.text\n",
    "    return text_to_analyze\n",
    "text_to_analyze = scrape_data(\"url\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code snippet provided, the following actions are performed:\n",
    "\n",
    "1. The necessary modules are imported: `openai` and `load_dotenv` from the `dotenv` library, and `os`.\n",
    "2. The `load_dotenv` function is used to load environment variables from a `.env` file.\n",
    "3. The `secret_key` variable is assigned the value of the environment variable named \"secret_key\" using `os.getenv()`.\n",
    "4. The OpenAI API key is set using the `api_key` attribute of the `openai` module.\n",
    "5. The `analyze_sentiment` function is defined. This function takes a text as input and performs sentiment analysis using the OpenAI completion API.\n",
    "6. Inside the `analyze_sentiment` function, a prompt is defined as \"This is a sentiment analysis task. The sentiment of the following text is:\".\n",
    "7. The input text is constructed by concatenating the prompt and the provided text.\n",
    "8. The OpenAI completion request is set up using the `openai.Completion.create()` method. The parameters include the engine, prompt, temperature, max_tokens, top_p, frequency_penalty, presence_penalty, and stop tokens.\n",
    "9. The completion response is obtained, and the sentiment label is extracted from the response.\n",
    "10. The sentiment label is returned as the result of the `analyze_sentiment` function.\n",
    "11. Below the code snippet, an example usage is shown. The `text` variable is assigned the value of `text_to_analyze`, which is the extracted text from web scraping.\n",
    "12. The `analyze_sentiment` function is called with the `text` variable as input, and the sentiment label is assigned to the `sentiment` variable.\n",
    "13. Finally, the sentiment label is printed.\n",
    "\n",
    "This code snippet demonstrates how to perform sentiment analysis using the OpenAI completion API by providing a prompt and input text. The sentiment label is then extracted from the completion response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraping_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
